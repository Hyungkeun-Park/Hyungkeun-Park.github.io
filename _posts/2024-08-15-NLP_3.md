---
title: "[밑시딥2] 단순 word2vec"
excerpt: "밑시딥2"

use_math: true

categories:
    - NLP
tags:
    - NLP
    - 밑시딥
last_modified_at: 2024-08-15T00:00:03
---

<!--bundle exec jekyll serve : 임시 확인-->

word2vec에서 사용되는 단순 신경망인 CBOW와 skip-gram에 대해 다뤄보겠습니다
<br>
<br>

# Continuous bag-of-words (CBOW)
---
CBOW 모델은 주변 맥락으로부터 해당 위치에 들어갈 단어를 추론하는 용도의 신경망입니다<br>
Ex) 입력받은 you와 goodbye라는 맥락 가운데 들어갈 단어를 추론<br>

입력으로 주변 맥락(you, goodbye)을 입력받아 은닉층의 벡터를 만든 뒤<br>
출력층의 벡터로 출력함으로써 해당 위치에 들어갈 가장 높은 확률의 단어를 유추하는 방식의 모델입니다<br>
은닉층의 경우, 입력받은 맥락으로부터 출력된 벡터를 모두 평균하는 방식을 택하고 있습니다<br>

해당 위치의 정답 단어를 추론하도록 학습(cross-entropy)하면<br>
은닉층으로의 가중치가 단어간의 분산 표현이 담긴다고 볼 수 있다고 합니다<br>
<br>
<br>

# Skip-gram
---
CBOW는 맥락 사이에 들어가는 정답 단어를 유추하는 방식이라면<br>
skip-gram은 반대로 정답 단어의 주변 맥락 단어를 추론하는 방식으로 학습을 진행하게 됩니다<br>
따라서 한 개의 입력을 받아 맥락 수 만큼의 출력을 하는 구조입니다<br>

각 출력에 대해 loss를 구하고, 모든 loss의 합으로 학습을 진행합니다<br>
<br>
<br>

**단어 분산 표현의 정밀도 면에서 skip-gram이 더 좋은 경우가 많다**고 합니다<br>
(아무래도 더 어려운 task에 대해 학습한 모델이기 때문이 아닐까 라고 합니다)