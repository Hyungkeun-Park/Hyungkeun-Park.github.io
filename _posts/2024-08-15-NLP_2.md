---
title: "[밑시딥2] 고전 NLP 방식"
excerpt: "밑시딥2"

use_math: true

categories:
    - NLP
tags:
    - NLP
    - 밑시딥
last_modified_at: 2024-08-15T00:00:02
---

<!--bundle exec jekyll serve : 임시 확인-->

단어간의 상관관계를 통계적으로 처리하기 위한 기법으로 쓰이던 몇 가지 방식을 정리해보고자 합니다
<br>
<br>

# 동시발생 행렬
---
동시발생 행렬의 경우, 특정 단어와 다른 단어가 얼마나 동시에 발생하고 있는지를 나타내줌으로써<br>
두 단어간의 관련성을 파악하는 방식입니다<br>

Ex) you say goodbye and i say hello.<br>
위 예시에서 you와 동시에 발생하는 단어 한 개만을 바라본다 하면 (윈도우=1)<br>
동시발생 행렬은 say라는 단어 외에는 0의 값을 갖게 될 것입니다<br>

위와 같은 방식으로 you, say, goodbye, ... 에 대한 동시발생 행렬을 구성하고<br>
각각의 단어에 대한 벡터간의 유사도를 구하면 두 단어간의 유사성을 어느정도 예측할 수 있습니다<br>

하지만 **윈도우의 크기**에 따라 유사도가 변화하고, **the와 같은 단어**에 대한 처리가 불가능해<br>
단어간의 정확한 유사도 분석이 불가능하다는 단점이 존재하여 다음과 같은 방식이 등장하게 됩니다<br>
<br>
<br>

# 점별 상호정보량(PMI), 양의 상호정보량(PPMI)
---
점별 상호정보량(Pointwise Mutual Information, PMI)는 다음과 같이 정의됩니다

$$
PMI(x,y)=log_2\frac{P(x,y)}{P(x)P(y)}
$$

x를 the, y를 car로 설정할 경우와, x를 drive, y를 car로 설정한다고 가정해 보면<br>

문장에서 상당히 많이 발생할 수 있는 the와, car의 상호정보량이<br>
car와 관련된 동사인 drive와의 상호정보량보다 작아지게 만들 수 있어집니다<br>

하지만 PMI는 log를 이용한 값으로, 두 단어가 동시 발생하지 않을 경우 음의 무한대 값을 갖게되기 때문에<br>
이를 해결하고자 양의 상호정보량(Positive PMI, PPMI)을 사용하였다고 합니다

$$
PPMI(x,y)=max(0,PMI(x,y))
$$

<br>
<br>

# 차원축소
---
PPMI를 통해 윈도우 크기에 따라 변화하는 유사도와, the같은 단어의 처리까지 해결하였지만<br>
학습시키고자 하는 문장이 상당히 길어질 경우, 각 단어간의 유사도를 모두 구해야하는 문제가 생기게 됩니다<br>

문장에 포함된 단어의 수가 10만개가 될 경우, 10만x10만 = 100억 차원의 벡터가 필요해지게 되는데 반해<br>
두 단어가 동시 발생하지 않는 0의 값 또한 그만큼 증가하여 행렬이 희소(sparse)해지기 시작합니다<br>

이런 문제를 해결하기 위해 SVD (Singular Value Decomposition)과 같은 방식을 통해 차원을 축소함으로써<br>
불필요한 희소 차원들을 줄일 수 있게 됩니다