---
title: "[PRML] 패턴인식과 머신 러닝 Ch1. 소개 - intro(1)"
#excerpt: ""

toc: true
toc_sticky: true

use_math: true

categories:
    - PRML
tags:
    - Deep Learning
    - Machine Learning
last_modified_at: 2023-07-11T16:00:00
---

>요하네스 케플러는 티코 브라헤가 축적해 놓은 대량의 천문학 데이터에서<br>패턴을 찾아내어 케플러의 행성 운동 법칙을 발견했다

이 책의 시작에서 저자는 위과 같은 예시를 들며 책을 시작한다

그리고 바로 손글씨 데이터 인식에 대해 예시를 들며 본격적으로 패턴 인식의 세계로 초대한다
<br>
<br>
# 예시: MNIST
* * *
손글씨 데이터인 MNIST 데이터셋은 28x28픽셀의 이미지로 784개의 실수로 구성된 벡터로 표현할 수 있다  
해당 데이터셋에서의 목표는 입력받은 실수 벡터 $X$를 숫자 0~9 중 하나의 값으로 출력해 내는 것이다<br>

이미지처리 라이브러리를 이용하여 각 <u>숫자를 인식하기 위한 규칙</u>을 만들어본다고 생각해보자  
벌써부터 <u>막막하기 그지없을 것</u>이다<br>

그렇기에 우리들은 머신 러닝을 이용하여 해당 데이터셋에서의 패턴을 찾으려고 시도한다<br>
<br>

데이터에 대한 패턴을 찾기 위해 학습에 사용하는 데이터를 `훈련 집합(Training set)`라고 하고<br>
그리고 학습된 패턴에 대해 테스트를 해보는 데이터를 `시험 집합(Test set)`라고 한다<br>
각 숫자 데이터에 대한 정답 카테고리는 `표적 벡터(target vector)`로 표현하며<br>
<br>

해당 데이터들을 이용해 `학습 단계(learning phase)`를 거쳐<br>
`입력 벡터 X`에 대한 `표적 T(target)`를 학습하게 된다<br>
손쉽게 표현해보면 머신 러닝은 `T=Y(X)`를 학습한다고 생각하면 된다
<br>
<br>
# 예시: 다항식 곡선 피팅
* * *
예시로 `sin(2πx)`함수를 이용해 입력변수 x에 대해 생성한 데이터에<br>
가우시안 노이즈를 추가한 타겟 데이터로 곡선을 피팅하는 예시를 들어보자<br>

```python
import numpy as np
import matplotlib.pyplot as plt

x=np.linspace(0,1,10)
y=np.sin(2*np.pi*x)
y_=y+np.random.normal(0,0.1,size=10)
plt.plot(x,y)
plt.scatter(x,y_,c='g')
```
![56bdb02c-e7df-4feb-b7cd-34f7424d2f38](https://github.com/Hyungkeun-Park/Hyungkeun-Park.github.io/assets/21329629/a5c24da9-4e98-487f-b96b-6e208ed645bc){: width="40%", height="40%" .align-center}

실제 데이터 집합인 파란색 곡석은 `sin(2πx)`라는 특정한 규칙을 가지지만<br>
관측값(초록 점)들은 보통 노이즈가 추가된 데이터들을 얻게된다<br>

훈련 집합(초록 점)들을 이용해 새로운 $\tilde{x}$가 입력됐을 때<br>
타겟 변수 $\tilde{t}$를 예측하는 것이 목적이다<br>

해당 곡선을 피팅하기 위해 다음과 같은 다항식을 활용할 것이다

$$
y(x,w)=w_0 + w_1x + w_2x^2 + \cdots + w_Mx^M=\displaystyle\sum_{j=0}^{M}{w_jx^j} \tag{1.1}
$$

위 다항식을 훈련 집합의 표적값과 함수값`y(x,w)`간의 오차를 최소화하는 방향으로 학습을 진행하며<br>
표적값과 함수값간의 오차를 정량화하는 함수를 `오차함수(error function)`라고 하며<br>
다항식의 차수 M을 결정하는 것을 `모델 비교(model comparison)` 또는 `모델 결정(model selection)`이라고 한다<br>
<br>

결정한 다항식의 계수를 결정하기 위해 가장 널리 쓰이는 오차 함수 중 하나인 `MSE(Mean Square Error)`는<br>
데이터 포인트 $x_n$에 대한 예측치 $y(x_n,w)$와 표적값 $t_n$ 사이의 오차를 제곱하여 합산하는 것이며

$$
E(w)=\frac{1}{2}\displaystyle\sum_{n-1}^N\\{y(x_n,w)-t_n\\}^2 \tag{1.2}
$$

![8b1e9f02-6efe-4e47-be41-1da76f4dde4f](https://github.com/Hyungkeun-Park/Hyungkeun-Park.github.io/assets/21329629/2b05834f-d35e-472d-806a-0e88a6c1d564){: width="40%", height="40%" .align-center}

기하학적으로 해석해보면 각각의 데이터 포인트와 예측값간의 직선 거리(빨간 선)의 제곱합이다<br>
<br>
위 오차 함수의 경우 이차 다항식의 형태를 지니고 있기 때문에 함수는 아래로 볼록한 모양을 띄며<br>
미분하여 0이 되는 지점에서 해당 오차가 최저가 됨을 알 수 있다<br>

## 과적합
`과적합(overfitting)`은 학습된 모델이 시험 집합에 대해서는 설명을 잘 못하는 현상을 일컫는다<br>
즉, 학습 집합에 대해서만 과하게 피팅됐다는 말이다<br>
<br>
우리는 학습 집합 이외의 데이터들에 대해서도 일반적으로 잘 맞는 모델을 학습하고 싶기 때문에<br>
<u>과적합을 피하는 것은 학습쟁이들의 숙명</u>이라고 할 수 있다<br>
<br>
과적합을 피하기 위해서는 학습에 필요한 양질의 많은 데이터를 준비하는 것이 좋겠으나<br>
학습 데이터를 무한정 얻어올 수 없으니 우리는 다른 방법에 대해서도 생각을 해보아야 한다<br>

1. 적당한 모델 차수 선택<br>
모델은 학습 집합에 대해 최대한 오차가 작아지는 방향으로 학습하기 때문에<br>
모델의 차수가 높을수록 엄청나게 기괴한 모양의 함수의 모양을 갖게 될 것이다<br>
<!--
<span style="color:yellow">**오차를 어느정도 허용하면서도 데이터 집합에 대해 잘 설명할 수 있는 차수를 선택하는 것이 중요하다<br>**</span>
-->
**오차를 어느정도 허용하면서도 데이터 집합에 대해 잘 설명할 수 있는 차수를 선택하는 것이 중요하다<br>**
2. 검증(validation) 집합<br>
모델의 차수를 적당하게 선택했는지를 단독으로 알 수 있는 방법은 없기 때문에<br>
`검증 집합`을 통해 차수 선택에 대해 적합도를 검증하며 훈련 집합의 일부를 사용하여 구성한다<br>
3. 정규화(regularization)<br>
`정규화`란 오차 함수에 특정 목적의 패널티를 주는 항을 추가하는 것을 뜻한다<br>
예를 들어, 모델 계수의 크기에 대한 항을 오차 함수에 추가할 수 있는데<br>

$$
E(w)=\frac{1}{2}\displaystyle\sum_{n-1}^N\\{y(x_n,w)-t_n\\}^2+\frac{\lambda}{2}\parallel{w}\parallel^2 \tag{1.4}
$$

위 식과 같은 이차 형식(quadratic)의 정규화를 `리지 회귀(Ridge regression)`<br>
일차 형식의 정규화를 `라쏘 회귀(Lasso regression)`라고 칭한다<br>
