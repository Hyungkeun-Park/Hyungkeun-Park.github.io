---
title: "[Review] InfoVAE: Balancing Learning and Inference in Variational Autoencoders"
#excerpt: ""

toc: true
toc_sticky: true

use_math: true

categories:
    - Review
tags:
    - Deep Learning
    - VAE
    - AutoEncoder
last_modified_at: 2023-07-03T01:00:00
---

작성중
{: .notice}

# Abstract
* * *
> VAE(Variational AutoEncoder)는 잘못 상각(amortized)된 분포를 학습할 가능성이 있고<br>
디코딩 분포가 너무 유연할 경우 잠재 변수를 무시하는 경향이 있다

라고 기존의 VAE의 한계점을 지적하며 해당 논문을 시작한다<br>
<br>
<br>

# 1. Introduction
* * *

$$
\log{p_\theta{x}}\geq-D_{KL}(q_\phi(z|x)\parallel{p(z)})+E_{q_\phi (z|x)}[\log{p_\theta{(x|z)}}]
$$

VAE의 경우 ***Evident Lower BOund(ELBO)***를 이용하여 Encoder, Decoder의 구조로 학습을 진행한다<br>
하지만, 모델의 용량 제한으로 인해 데이터 분포에 피팅되는 경향이 있고<br>
단일 조건부 분포만을 사용하기에 잠재 변수를 무시하고 VAE의 혼합 모델링의 이점을 취하지 못한다고 한다<br>
<br>
위 문제를 해결하기 위해 추론과 데이터 분포 피팅간의 가중치를 줄 수 있는 목적 함수를 제안한다고 한다<br>
<br>
<br>

# 2. Variational Autoencoders
* * *
VAE는 특징 차원 Z와 입력 데이터 차원 X의 결합 분포에 대해 모델링하는 것으로<br>
Z는 가우시안과 같은 분포로 가정하고 데이터 차원 X를 복잡한 조건부 분포인 $p_{\theta}{(x\|z)}$로 모델링하여 학습한다<br>

여기서 트레이닝 데이터에 대한 분포를 $p_D(x)$라고 할 때의 ***ML(Maximum Likelihood)***는<br>
기존 VAE에 대해 데이터 분포에 대한 기대값만 추가된 형태로 다음과 같다<br>

$$
E_{p_D(x)}[\log{p_\theta{x}}]=E_{p_D(x)}[\log{E_{p(z)}[p_\theta{(x\|z)}]}]
$$

위 식과 $\theta$와 $\phi$에 대한 두 네트워크에서의 결합분포

$$
p_\theta{(x,z)}≡p(z)p_\theta(x\|z)\qquad
q_\phi(x,z)≡p_D(x)q_\phi(z\|x)
$$

그리고 기존의 VAE의 식을 이용하면 다음과 같은 ELBO식을 얻을 수 있다고 한다 ~~(3, 4는 어떻게 나온걸까...?)~~<br>

$$
L_{ELBO}≡-D_{KL}(q_\phi{(x,z)\parallel(p_\theta{(x,z)})}) \tag{2} 
$$

$$
=-D_{KL}{(p_D{(x)}\parallel{p_\theta{(x)}})}-E_{p_D{(x)}}[{D_{KL}{(q_\phi{(z\|x)}\parallel{p_\theta{(z\|x)}})}}] \tag{3}
$$

$$
=-D_{KL}{(q_\phi{(z)}\parallel{p{(z)}})}-E_{q_\phi{(z)}}[{D_{KL}{(q_\phi{(x\|z)}\parallel{p_\theta{(x\|z)}})}}] \tag{4}
$$

<br>
<br>

# 3. Two Problems of Variational Autoencoders
* * *