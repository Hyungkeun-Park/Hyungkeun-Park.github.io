---
title: "[Review] Non-Uniform Step Size Quantization for Accurate Post-Training Quantization, ECCV, 2022"
excerpt: "Non-uniform quantization"

toc: true
toc_sticky: true

use_math: true

categories:
    - Quantization
tags:
    - 경량화
    - Knowledge Distillation
    - Quantization
last_modified_at: 2024-07-20T01:00:00
---

<!--bundle exec jekyll serve : 임시 확인-->

**[Non-Uniform Step Size Quantization for Accurate Post-Training Quantization](https://aigs.unist.ac.kr/filebox/item/1917192674_f1cd20ad_136710657.pdf)**<br>
<br>
<br>

해당 논문에서는<br>
**`Logarithmic-scale quantization`**을 이용하여 **$arithmetic precision > representational precision$**이 되도록 하였다고 함

