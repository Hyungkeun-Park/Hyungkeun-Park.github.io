---
title: "[밑시딥2] 진짜 word2vec"
excerpt: "밑시딥2"

use_math: true

categories:
    - NLP
tags:
    - NLP
    - 밑시딥
last_modified_at: 2024-08-15T00:00:04
---

<!--bundle exec jekyll serve : 임시 확인-->

이전 장에서는 각 단어에 해당하는 one-hot 벡터로 학습시키는 CBOW, skip-gram 방식을 다루었는데<br>

단순 word2vec에
1. Embedding 계층 도입
2. 네거티브 샘플링 loss 도입
을 통해 **`진짜`** word2vec을 완성할 수 있다고 합니다<br>
<br>
<br>

# Embedding
---
앞장에서는 입력 단어에 대한 one-hot 벡터로 모델에 입력을 하는 예시를 들어 진행하였는데<br>
요즘의 LLM과 같은 모델을 학습시키기 위한 말도안되는 양의 데이터를 사용할 경우<br>
one-hot 벡터의 차원 또한 어마무시해져 은닉층의 출력을 구하는데에도 엄청난 연산이 필요해집니다<br>
Ex) 100만 단어 → 1x100만 x 100만x은닉층 뉴런 수 → 1x은닉층 뉴런 수<br>

하지만 위 연산을 생각해보면 **특정 행의 가중치 값을 가져오는 행위**를 위해<br>
행렬곱 연산을 한다는 것을 알 수 있습니다<br>

그렇다면 특정 단어에 해당하는 ID 값을 안다면, 해당하는 가중치를 가져오게끔만 함으로써<br>
상당한 연산을 줄여낼 수 있음을 알 수 있습니다
<br>
<br>

# Negative sampling
---
정답 단어의 추론을 위해 softmax를 사용할 경우<br>
벡터의 크기가 커질수록 동일하게 연산량이 많아진다는 문제점이 발생하게 됩니다<br>
이를 해결하기 위해 **다중 분류 문제를 이진 분류 문제로 근사**시킵니다<br>

Ex) you와 goodbye 사이에 들어갈 단어는? → 사이에 들어갈 단어는 say인가?<br>

위와 같이 변경시 say에 해당하는 가중치와의 연산만 진행하면 되기 때문에<br>
시간적, 메모리적 이득을 상당히 볼 수 있게 됩니다<br>

그리고 say가 아닌 단어에 대해 모두 이진 분류 학습을 시키는 것이 아닌<br>
특정 몇 개만을 선택하여 학습하는 `네거티브 샘플링` 기법을 이용합니다<br>
이 때, 특정 몇 개의 단어를 선택하는 샘플링의 기준은 출현 횟수를 이용한다고 합니다<br>